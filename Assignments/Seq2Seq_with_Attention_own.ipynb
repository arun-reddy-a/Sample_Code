{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torchdata.datapipes as dp\n",
    "import torchtext.transforms as T\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import spacy\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(folder_path, file1_name, file2_name):\n",
    "    # Initialize an empty dictionary\n",
    "    data = []\n",
    "\n",
    "    # Read the contents of both files and populate the dictionary\n",
    "    with open(f\"{folder_path}/{file1_name}\", 'r') as file1, open(f\"{folder_path}/{file2_name}\", 'r') as file2:\n",
    "        lines_file1 = file1.readlines()\n",
    "        lines_file2 = file2.readlines()\n",
    "\n",
    "        # Assuming both files have the same number of lines\n",
    "        for line1, line2 in zip(lines_file1, lines_file2):\n",
    "            if line1.strip() != '' and line2.strip() != '':\n",
    "                sentence1 = line1.strip()\n",
    "                sentence2 = line2.strip()\n",
    "                data.append((sentence1, sentence2))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = get_data('translation_dataset/training', 'train.de', 'train.en')\n",
    "val_data = get_data('translation_dataset/validation/', 'val.de', 'val.en')\n",
    "test_data = get_data('translation_dataset/testing/', 'test.de', 'test.en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing The Data into Datapipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dp.iter.IterableWrapper(train_data)\n",
    "val_data = dp.iter.IterableWrapper(val_data)\n",
    "test_data = dp.iter.IterableWrapper(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove This Line\n",
    "train_data = test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Ein Mädchen an einer Küste mit einem Berg im Hintergrund.',\n",
       " 'A girl at the shore of a beach with a mountain in the distance.')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(test_data)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_de = spacy.load('/Users/arunreddy/opt/anaconda3/envs/NLP_PYTORCH/lib/python3.8/site-packages/spacy/data/de')\n",
    "spacy_en = spacy.load('/Users/arunreddy/opt/anaconda3/envs/NLP_PYTORCH/lib/python3.8/site-packages/spacy/data/en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eng_tokenizer(text):\n",
    "    return [tok.text for tok in spacy_en(text)]\n",
    "\n",
    "def de_tokenizer(text):\n",
    "    return [tok.text for tok in spacy_de(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTokens(data_iter, place):\n",
    "    for german, english in data_iter:\n",
    "        if place == 0:\n",
    "            yield de_tokenizer(german)\n",
    "        else:\n",
    "            yield eng_tokenizer(english)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_de = build_vocab_from_iterator(getTokens(train_data,0), min_freq=2, specials=[\"<sos>\", \"<eos>\", \"<unk>\", \"<pad>\"], special_first=True)\n",
    "vocab_en = build_vocab_from_iterator(getTokens(train_data,1), min_freq=2, specials=[\"<sos>\", \"<eos>\", \"<unk>\", \"<pad>\"], special_first=True)\n",
    "vocab_de.set_default_index(vocab_de[\"<unk>\"])\n",
    "vocab_en.set_default_index(vocab_en[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<sos>', '<eos>', '<unk>', '<pad>', '.', 'einem', 'Ein', 'in', 'auf', 'und']\n"
     ]
    }
   ],
   "source": [
    "print(vocab_de.get_itos()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<sos>', '<eos>', '<unk>', '<pad>', 'a', '.', 'A', 'in', 'the', 'on']\n"
     ]
    }
   ],
   "source": [
    "print(vocab_en.get_itos()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numericalize sentences using the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTransform(vocab, sentence):\n",
    "    sentence_transform = T.Sequential(\n",
    "        T.VocabTransform(vocab=vocab),\n",
    "        T.AddToken(0, begin=True),\n",
    "        T.AddToken(1, begin=False)\n",
    "    )\n",
    "    if (vocab==vocab_de):\n",
    "        tokenized_text =[tok.text for tok in spacy_de(sentence)]\n",
    "    else:\n",
    "        tokenized_text =[tok.text for tok in spacy_en(sentence)]\n",
    "    return sentence_transform(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInvTransform(vocab, sentence):\n",
    "    result = \"\"\n",
    "    for i in sentence:\n",
    "        result += vocab.get_itos()[i] + \" \"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_de = list(train_data)[10][0]\n",
    "sentence_en = list(train_data)[10][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Eine Mutter und ihr kleiner Sohn genießen einen schönen Tag im Freien.',\n",
       " 'A mother and her young song enjoying a beautiful day outside.')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_de, sentence_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 14, 575, 9, 338, 87, 2, 694, 18, 348, 179, 17, 57, 4, 1]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getTransform(vocab_de, sentence_de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 6, 705, 10, 55, 26, 529, 302, 4, 359, 155, 49, 5, 1]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getTransform(vocab_en, sentence_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ApplyTransform(sequence_pair):\n",
    "\n",
    "    return (getTransform(vocab_de, sequence_pair[0]), getTransform(vocab_en, sequence_pair[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_transformed = train_data.map(ApplyTransform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Ein Mann mit einem orangefarbenen Hut, der etwas anstarrt.', 'A man in an orange hat starring at something.')\n"
     ]
    }
   ],
   "source": [
    "for sample in train_data:\n",
    "    print(sample)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([0, 6, 11, 10, 5, 151, 225, 13, 20, 115, 2, 4, 1], [0, 6, 12, 7, 28, 94, 86, 2, 19, 134, 5, 1])\n"
     ]
    }
   ],
   "source": [
    "for sample in train_data_transformed:\n",
    "    print(sample)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortBucket(bucket):\n",
    "    \"\"\"\n",
    "    Function to sort a given bucket. Here, we want to sort based on the length of\n",
    "    source and target sequence.\n",
    "    \"\"\"\n",
    "    return sorted(bucket, key=lambda x: (len(x[0]), len(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_transformed = train_data_transformed.bucketbatch(\n",
    "    batch_size = 4, batch_num=5,  bucket_num=1,\n",
    "    use_in_batch_shuffle=False, sort_key=sortBucket\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([0, 6, 44, 2, 17, 73, 4, 1], [0, 6, 47, 11, 530, 7, 8, 52, 1]), ([0, 6, 11, 347, 2, 25, 173, 4, 1], [0, 6, 12, 625, 2, 14, 212, 5, 1]), ([0, 58, 13, 22, 27, 5, 94, 49, 4, 1], [0, 58, 34, 49, 14, 4, 76, 5, 1]), ([0, 14, 2, 16, 36, 8, 5, 2, 4, 1], [0, 6, 2, 16, 108, 4, 2, 5, 1])]\n"
     ]
    }
   ],
   "source": [
    "for sample in train_data_transformed:\n",
    "    print(sample)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generally, while training a model,\n",
    "# we predict on a batch of X and compare the result with y. \n",
    "# But, a batch in our data_pipe is of the form [(X_1,y_1), (X_2,y_2), (X_3,y_3), (X_4,y_4)]:\n",
    "# So, we will now convert them into the form: ((X_1,X_2,X_3,X_4), (y_1,y_2,y_3,y_4)). For this we will write a small function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separateSourceTarget(sequence_pairs):\n",
    "    \"\"\"\n",
    "    input of form: `[(X_1,y_1), (X_2,y_2), (X_3,y_3), (X_4,y_4)]`\n",
    "    output of form: `((X_1,X_2,X_3,X_4), (y_1,y_2,y_3,y_4))`\n",
    "    \"\"\"\n",
    "    sources,targets = zip(*sequence_pairs)\n",
    "    return sources,targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_transformed = train_data_transformed.map(separateSourceTarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([0, 93, 91, 140, 118, 8, 22, 2, 4, 1], [0, 315, 58, 106, 2, 70, 21, 2, 4, 1], [0, 254, 80, 49, 7, 242, 2, 8, 5, 583, 4, 1], [0, 6, 11, 40, 61, 2, 2, 7, 12, 256, 4, 1])\n",
      "\n",
      "\n",
      "([0, 45, 26, 122, 11, 48, 19, 8, 2, 5, 1], [0, 347, 24, 207, 107, 436, 67, 4, 2, 2, 5, 1], [0, 291, 113, 15, 34, 9, 4, 730, 7, 99, 2, 534, 5, 1], [0, 6, 12, 11, 34, 46, 4, 43, 14, 551, 2, 7, 4, 217, 5, 1])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for batch in train_data_transformed:\n",
    "    for sample in batch:\n",
    "        print(sample) \n",
    "        print(\"\\n\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyPadding(pair_of_sequences):\n",
    "    \"\"\"\n",
    "    Convert sequences to tensors and apply padding\n",
    "    \"\"\"\n",
    "    return (T.ToTensor(3)(list(pair_of_sequences[0])), T.ToTensor(3)(list(pair_of_sequences[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_transformed = train_data_transformed.map(applyPadding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying The Final Processed Data Form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 13])\n",
      "torch.Size([4, 14])\n"
     ]
    }
   ],
   "source": [
    "for sources, targets in train_data_transformed:\n",
    "    print(sources.shape)\n",
    "    print(targets.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep this in mind the loop variables in python can leak out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The source sample 0 is  <sos> Eine Frau in einem pinken Pulli und einer Schürze <unk> einen Tisch mit einem <unk> . <eos> <pad> <pad> <pad> \n",
      "The target sample 0 is  <sos> A woman in a pink sweater and an apron , <unk> a table with a <unk> . <eos> <pad> <pad> <pad> <pad> \n",
      "-----------------------------------------\n",
      "The source sample 1 is  <sos> Ein Mann drückt den Kopf eines anderen Mannes nach unten und <unk> ihm ins Gesicht <unk> . <eos> <pad> <pad> \n",
      "The target sample 1 is  <sos> One man holds another man 's head down and prepares to <unk> him in the face . <eos> <pad> <pad> <pad> <pad> \n",
      "-----------------------------------------\n",
      "The source sample 2 is  <sos> Drei Leute sitzen an einem Picknicktisch vor einem Gebäude , das wie der <unk> <unk> <unk> ist . <eos> <pad> \n",
      "The target sample 2 is  <sos> Three people sit at a picnic table outside of a building painted like a <unk> <unk> . <eos> <pad> <pad> <pad> <pad> \n",
      "-----------------------------------------\n",
      "The source sample 3 is  <sos> Ein Angestellter reicht einer Frau auf einem Markt eine <unk> , während sie auf Eis <unk> Fisch <unk> . <eos> \n",
      "The target sample 3 is  <sos> An employee is <unk> a woman a bag while she is <unk> through fish on ice at a street market . <eos> \n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for sources, targets in train_data_transformed:\n",
    "    batch_size = sources.shape[0]\n",
    "    for i in range(batch_size):\n",
    "        source = sources[i]\n",
    "        target = targets[i]\n",
    "        source_sentence = getInvTransform(vocab_de, source.numpy())\n",
    "        target_sentence = getInvTransform(vocab_en, target.numpy())\n",
    "        print(f\"The source sample {i} is \", source_sentence)\n",
    "        print(f\"The target sample {i} is \", target_sentence)\n",
    "        print(\"-----------------------------------------\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally Data Preprocessing is Done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__ (self, enc_vocab_size, enc_embedding_size, enc_hid_dim,dec_hid_dim, enc_n_layers, dec_n_layers, enc_drop):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(enc_vocab_size, enc_embedding_size)\n",
    "        self.rnn = nn.LSTM(enc_embedding_size, enc_hid_dim, enc_n_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc_hidden = nn.Linear(enc_hid_dim*2, dec_hid_dim)\n",
    "        self.fc_cell = nn.Linear(enc_hid_dim*2, dec_hid_dim)\n",
    "        self.dec_n_layers = dec_n_layers\n",
    "        self.dropout = nn.Dropout(enc_drop)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x->(N, L)\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "\n",
    "        # embedded-> (N, L, H_enc_em)\n",
    "\n",
    "        output, (hidden, cell) = self.rnn(embedded)\n",
    "\n",
    "        # output->(N, L, 2*enc_hid_dim)\n",
    "        # hidden, cell->(2*enc_n_layers,N,enc_hid_dim)\n",
    "\n",
    "        context_hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "\n",
    "        context_cell = torch.cat((cell[-2,:,:], cell[-1,:,:]), dim=1)\n",
    "\n",
    "        # context_hidden->(N, enc_hid_dim*2)\n",
    "        # context_cell->(N, enc_hid_dim*2)\n",
    "\n",
    "        init_dec_hidden = self.relu(self.fc_hidden(context_hidden)).unsqueeze(0).repeat(self.dec_n_layers, 1,1)\n",
    "\n",
    "        init_dec_cell = self.relu(self.fc_cell(context_cell)).unsqueeze(0).repeat(self.dec_n_layers, 1, 1)\n",
    "\n",
    "        # init_dec_hidden, init_dec_cell->(dec_n_layers, N, dec_hid_dim)\n",
    "\n",
    "        return output, init_dec_hidden, init_dec_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 12, 256]), torch.Size([5, 10, 128]), torch.Size([5, 10, 128]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Encoder(13, 18, 128, 128,5, 5,0.1)\n",
    "inputs = torch.randint(0,13, (10, 12))\n",
    "output, init_dec_hidden, init_dec_cell = encoder(inputs)\n",
    "output.shape, init_dec_hidden.shape, init_dec_cell.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W = nn.Linear(2*enc_hid_dim+dec_hid_dim+dec_hid_dim, dec_hid_dim, bias=False)\n",
    "        self.v = nn.Linear(dec_hid_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, enc_hidden_states, prev_dec_hidden_state, prev_dec_cell_state):\n",
    "        \n",
    "        # enc_hidden_states -> (N, L, 2*enc_hid_dim)\n",
    "        # prev_dec_hidden_state -> (dec_n_layers, N, dec_hid_dim)\n",
    "        # prev_dec_cell_state -> (dec_n_layers, N, dec_hid_dim)\n",
    "\n",
    "        seq_length = enc_hidden_states.shape[1]\n",
    "        prev_dec_hidden_context = prev_dec_hidden_state[-1].unsqueeze(1).repeat(1, seq_length, 1)\n",
    "        prev_dec_cell_context = prev_dec_cell_state[-1].unsqueeze(1).repeat(1, seq_length, 1)\n",
    "\n",
    "        # prev_dec_hidden_context -> (N, L, dec_hid_dim)\n",
    "        # prev_dec_cell_context -> (N, L, dec_hid_dim)\n",
    "\n",
    "        combined_state = torch.cat((enc_hidden_states, prev_dec_hidden_context, prev_dec_cell_context), dim=2)\n",
    "\n",
    "        # combined_state -> (N, L, 2*enc_hid_dim + dec_hid_dim + dec_hid_dim)\n",
    "\n",
    "        un_norm_attn_scores = self.v(self.W(combined_state))\n",
    "\n",
    "        # un_norm_attn_scores -> (N, L, 1)\n",
    "\n",
    "        return F.softmax(un_norm_attn_scores.squeeze(2), dim=1) # -> (N, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = BahdanauAttention(128, 128)\n",
    "enc_hidden_states = torch.randn(10, 12, 2*128)\n",
    "prev_dec_hidden_state = torch.randn(5, 10, 128)\n",
    "prev_dec_cell_state = torch.randn(5, 10, 128)\n",
    "scores = attn(enc_hidden_states, prev_dec_hidden_state, prev_dec_cell_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 12])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, dec_hid_dim, dec_n_layers, enc_hid_dim, input_dim, dec_embedding_size, dec_drop, attention):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.attention = attention\n",
    "        self.dropout = nn.Dropout(dec_drop)\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, dec_embedding_size)\n",
    "        self.rnn = nn.LSTM(2*enc_hid_dim+dec_embedding_size, dec_hid_dim, dec_n_layers,batch_first=True)\n",
    "        self.fc_out = nn.Linear(2*enc_hid_dim+dec_embedding_size+dec_hid_dim, output_dim)\n",
    "\n",
    "    def forward (self, input, prev_dec_hidden_state, prev_dec_cell_state, enc_hidden_states):\n",
    "\n",
    "        # input -> (N, 1)\n",
    "        # prev_dec_hidden_state = prev_dec_cell_state -> (dec_n_layers, N, dec_hid_dim)\n",
    "        # enc_hidden_states -> (N, L, 2*enc_hid_dim)\n",
    "\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        # embedded->(N, 1, dec_embedding_size)\n",
    "        \n",
    "        # calculating the context vector.\n",
    "        attn_scores = self.attention(enc_hidden_states, prev_dec_hidden_state, prev_dec_cell_state)\n",
    "\n",
    "        # attn_scores->(N, L)\n",
    "        context_vector = torch.bmm(attn_scores.unsqueeze(1), enc_hidden_states)\n",
    "\n",
    "        # context_vector-> (N, 1, 2*enc_hid_dim)\n",
    "\n",
    "        rnn_input = torch.cat((embedded, context_vector), dim=2)\n",
    "        # rnn_input->(N, 1, 2*enc_hid_dim+dec_embedding_size)\n",
    "\n",
    "        output, (hidden, cell) = self.rnn(rnn_input, (prev_dec_hidden_state, prev_dec_cell_state))\n",
    "\n",
    "        # output->(N, 1, dec_hid_dim)\n",
    "        # hidden = cell->(dec_n_layers, N, dec_hid_dim)\n",
    "\n",
    "\n",
    "        prediction = self.fc_out(torch.cat((embedded, context_vector, output), dim=2).squeeze(1))\n",
    "        # prediction->(N, output_dim)\n",
    "\n",
    "        return prediction, (hidden, cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "274"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "256+18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(200, 128, 5, 128, 200, 18, 0.2, attn)\n",
    "input = torch.randint(0, 200, (10,1))\n",
    "prediction, (hidden, cell) = decoder(input, prev_dec_hidden_state, prev_dec_cell_state, enc_hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 200]), torch.Size([5, 10, 128]), torch.Size([5, 10, 128]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.shape, hidden.shape, cell.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src, trg, teacher_force_ratio):\n",
    "\n",
    "        # src->(N, L)\n",
    "        # trg->(N, L')\n",
    "\n",
    "        batch_size = src.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        dec_outputs = torch.zeros(trg_len, batch_size, self.decoder.output_dim)\n",
    "        output, init_dec_hidden, init_dec_cell = self.encoder(src)\n",
    "\n",
    "        input = trg[:, 0]\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            prediction, (hidden, cell) = self.decoder(input.unsqueeze(1), init_dec_hidden, init_dec_cell, output)\n",
    "            # prediction->(N->output_dim)\n",
    "            # hidden = cell -> (dec_n_layers, N, dec_hid_dim)\n",
    "            dec_outputs[t] = prediction\n",
    "            teacher_force = random.random() < teacher_force_ratio\n",
    "            top1 = prediction.argmax(1)\n",
    "            input = trg[:, t] if teacher_force else top1\n",
    "\n",
    "        return dec_outputs.permute(1,0,2) #-> (N, L, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2s = Seq2Seq(encoder, decoder)\n",
    "dec_outputs = s2s(torch.randint(0,5,(10,14)), torch.randint(0,5,(10,25)), 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 25, 200])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_vocab_size = len(vocab_de.get_itos())\n",
    "enc_embedding_size = 25\n",
    "enc_hid_dim = 25\n",
    "dec_hid_dim = 25\n",
    "enc_n_layers = 1\n",
    "dec_n_layers = 1\n",
    "enc_drop = 0.5\n",
    "output_dim = len(vocab_en.get_itos())\n",
    "input_dim = len(vocab_en.get_itos())\n",
    "dec_embedding_size = 25\n",
    "dec_drop = 0.5\n",
    "dec_n_layers = 1\n",
    "actual_encoder = Encoder(enc_vocab_size, enc_embedding_size, enc_hid_dim, dec_hid_dim, enc_n_layers, dec_n_layers, enc_drop)\n",
    "actual_attn = BahdanauAttention(enc_hid_dim, dec_hid_dim)\n",
    "actual_decoder = Decoder(output_dim, dec_hid_dim, dec_n_layers, enc_hid_dim, input_dim, dec_embedding_size, dec_drop, actual_attn)\n",
    "actual_s2s = Seq2Seq(actual_encoder, actual_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(actual_s2s.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab_en.get_stoi()[\"<pad>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for i, batch in tqdm(enumerate(iterator)):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        source = batch[0] # -> (N, L)\n",
    "        target = batch[1] # -> (N, L')\n",
    "\n",
    "        predictions = model(source, target, 0.5) #-> (N, L', output_dim)\n",
    "        \n",
    "        target = target[:, 1:].reshape(-1) # -> (N, L'-1) -> (N*(L'-1))\n",
    "        predictions = predictions[:,1:,:].reshape(-1, output_dim) #-> (N, L', output_dim) -> (N*(L'-1), output_dim)\n",
    "\n",
    "        loss = criterion(predictions, target)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(epoch_loss/i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "250it [00:16, 15.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.2009725781329665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "250it [00:15, 15.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.603021787352351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "250it [00:16, 15.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.4815530566326585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "250it [00:16, 14.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.374613053348648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "250it [00:17, 14.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.293427253822725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "250it [00:17, 14.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.220458491260268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "250it [00:17, 14.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.162144240605304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "250it [00:17, 14.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.105428957077394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "250it [00:17, 14.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.055018787881935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "250it [00:17, 14.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9810871321513472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    train(actual_s2s, train_data_transformed, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = [5.2009725781329665, 4.603021787352351, 4.4815530566326585, 4.374613053348648, 4.293427253822725, 4.220458491260268\n",
    ", 4.162144240605304\n",
    ", 4.105428957077394\n",
    ", 4.055018787881935\n",
    ", 3.9810871321513472]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as ply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f97401fda30>]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ply.plot(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_PYTORCH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
